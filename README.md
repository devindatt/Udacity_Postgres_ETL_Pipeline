## DEND Project:  Data Modeling with Postgres
[![N|Solid](https://cdn.sisense.com/wp-content/uploads/logo-connector-postgresql1.png)](https://www.postgresql.org)


### Project Description
A startup called Sparkify wants to analyze the data they've been collecting on songs and user activity which resides in a directory of JSON logs on user activity on the app, as well as a directory with JSON metadata on the songs in their app.

In this scenario, we acting as the data engineer on the team are to create a Postgres database with tables designed to optimize queries on song play analysis. We are to create a database schema and ETL pipeline for this analysis. Also to test our database and ETL pipeline by running queries given to us by the analytics team from Sparkify and compare our results with their expected results.

Tasks are to:
- Create a database schema and ETL pipeline for the analysis of these data and log files
- Test the database and ETL pipeline by running queries
- Compare these results with Sparkify's expected results
- Define fact and dimension tables for a star schema for a particular analytic focus
- Create an ETL pipeline that transfers data from files in two local directories into these tables in Postgres using Python and SQL

##### Song Dataset
The Song dataset is a subset of real data from the Million Song Dataset. Each file is in JSON format and contains metadata about a song and the artist of that song. The files are partitioned by the first three letters of each song's track ID

##### Log Dataset
The Log dataset consists of log files in JSON format generated by this event simulator based on the songs in the dataset above. These simulate app activity logs from a music streaming app based on specified configurations.The files are partitioned by year and month.

##### Schema for Song Play Analysis
Using the song and log datasets, I created a star schema optimized for queries on song play analysis. This includes the following tables. Added primary keys to 

##### Fact Table
**songplays** - records in log data associated with song plays i.e. records with page NextSong
-- *songplay_id SERIAL PRIMARY KEY, start_time time NOT NULL, user_id varchar NOT NULL, 
level varchar, song_id varchar, artist_id varchar, session_id int, location varchar, user_agent varchar*

##### Dimension Tables
**users** - users in the app
-- *user_id, first_name, last_name, gender, level*

**songs** - songs in music database
-- *song_id, title, artist_id, year, duration*

**artists** - artists in music database
 -- *artist_id, name, location, lattitude, longitude*

**time** - timestamps of records in songplays broken down into specific units
-- start_time, hour, day, week, month, year, weekday

-------------------------------------------------------------------------
The data files included in this project are the six files below:

| File | Description |
| ------ | ------ |
| test.ipynb | Displays the first few rows of each table to let you check your database |
| create_tables.py | Drops and creates your tables. Run this file to reset the tables before each time running your ETL scripts |
| etl.ipynb | Reads and processes a single file from song_data and log_data and loads the data into the tables |
| etl.py | Reads and processes files from song_data and log_data and loads them into the tables |
|sql_queries.py | Contains all of the used sql queries |

 
#### Project Steps
Below are steps I took to complete the project:

(1) **Create Tables**
-- Completed CREATE statements in *sql_queries.py* to create each table
-- Write DROP statements in sql_queries.py to drop each table if it exists
-- Run *create_tables.py* to create your database and tables
-- Run test.ipynb to confirm the creation of my tables with the correct columns

(2) **Build ETL Processes**
-- Develop ETL processes for each table
-- Ran *test.ipynb* to confirm that records were successfully inserted into each table

(3) **Build ETL Pipeline**
-- Created from etl.ipynb to complete etl.py pipeline that process the entire datasets
-- Ran test.ipynb to confirm my records were successfully inserted into each table

##### Example of Final Table Output

-- This table a compilation of the three (3) tables (songs, artists, activity logs)
![N|Solid](https://s3.amazonaws.com/video.udacity-data.com/topher/2019/February/5c6c15e9_log-data/log-data.png)


-------------------------------------------------------------------------
